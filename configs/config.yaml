defaults:
- _self_
- paths: default
- data: train1k_val20
- model: reprover_full_finetune
- ntp: default
- reward: default

seed: 27
save_dir: runs

trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: cuda
  max_epochs: 2
  accumulate_grad_batches: 1
  val_check_interval: 20
  gradient_clip_val: 0.5
  num_sanity_val_steps: 0
  log_every_n_steps: 1

  logger:
    _target_: pytorch_lightning.loggers.WandbLogger
    name: ${now:%Y-%m-%d}_${now:%H-%M-%S}
    project: gfn4ntp
    save_dir: ${save_dir}
    offline: False

  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: val/logR
      mode: max
      save_last: true
      dirpath: ${save_dir}/checkpoints/${now:%Y-%m-%d}_${now:%H-%M-%S}
      filename: epoch={epoch:03d}
      auto_insert_metric_name: true
    # - _target_: pytorch_lightning.callbacks.EarlyStopping
    #   monitor: val/logR
    #   mode: max
    #   patience: 10

shared_values:
  repeat_theorem_n_times: 1
  architecture: seq2seq # {seq2seq, causal}
  seq2seq: ${arch2seq2seq[${architecture}]}
  use_4bit: false
  
# for variable interpolation
arch2seq2seq:
  seq2seq: true
  causal: false
