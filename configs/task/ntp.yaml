name: "ntp"

data:
  path: "data/time_filtered_v2.json"
  train_size: 0.95

model:
  name: "deepseek-ai/DeepSeek-Prover-V1"
  lora_config:
    _target_: peft.LoraConfig
    target_modules: "all-linear"
    r: 64
    lora_alpha: 16
    lora_dropout: 0.1
    bias: "none"
    fan_in_fan_out: true

training:
  # subtb_lambda: 1.0
  pf_temp_high: 2.0
  pf_temp_low: 0.5
  pf_temp_prob: 0.666
  use_buffer_prob: 0.25
  n_samples: 8  # next_sentence task had 20
  lr: 0.0001
  accumulate_grad_batches: 25
  epochs: 1 # next_sentence task had 300
  use_4bit: false
  use_replay_tree: false

eval:
  n_probes: 10
  diversity_metric: "sequence_embedding"

reward:
  temp_start: 1.0
  temp_end: 0.8
  temp_horizon: 750
  vocab_alpha: -50
  sentence_validator: null
  buffer_size: 50
  buffer_sim_tolerance: 0.25
  verifier_batch_size: 16 # TODO: figure out if we can go higher

callbacks:
  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/logR"
    mode: "max"
    save_last: true
    dirpath: ${save_dir}/checkpoints/${now:%Y-%m-%d}_${now:%H-%M-%S}
    filename: "epoch={epoch:03d}"
    auto_insert_metric_name: true
  - _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/logR"
    mode: "max"
    patience: 10
  # next_sentence settings:
  # - _target_: pytorch_lightning.callbacks.ModelCheckpoint
  #   monitor: "val/logP(s) (avg)"
  #   mode: "max"
  #   save_last: true
  #   dirpath: ${save_dir}/checkpoints/${now:%Y-%m-%d}_${now:%H-%M-%S}
  #   filename: "epoch={epoch:03d}"
  #   auto_insert_metric_name: true
  # - _target_: pytorch_lightning.callbacks.EarlyStopping
  #   monitor: "val/logP(s) (avg)"
  #   mode: "max"
  #   patience: 10

constraints:
  max_tactics: 3
  min_tactic_tokens: 1
  max_tactic_tokens: 30
